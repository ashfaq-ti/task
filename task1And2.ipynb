{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b67888-e67c-4b51-8b78-f0e1e1c83a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '14', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b51984-0cc3-42fb-9598-b99cfe7235d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/technoidentity/Downloads/gemma-2b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: special tokens cache size = 388\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 601\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=\"/home/technoidentity/Downloads/gemma-2b.gguf\",\n",
    "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79d0acd-5155-440f-80ad-d8868a7b1639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1598.10 ms\n",
      "llama_print_timings:      sample time =      89.93 ms /    14 runs   (    6.42 ms per token,   155.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1506.93 ms /    10 tokens (  150.69 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:        eval time =    9030.78 ms /    13 runs   (  694.68 ms per token,     1.44 tokens per second)\n",
      "llama_print_timings:       total time =   10662.62 ms /    23 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-9b4fc0dc-f61a-4756-b62f-eada78ff5857', 'object': 'text_completion', 'created': 1720596917, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': 'Q: name 3 species of fish? A: 1. bluegill 2. crappie 3. perch', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 14, 'total_tokens': 27}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: name 3 species of fish? A: \", # Prompt\n",
    "      max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de41819-5e68-442b-93c5-7c7cc34f5c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1598.10 ms\n",
      "llama_print_timings:      sample time =     289.47 ms /    39 runs   (    7.42 ms per token,   134.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3005.81 ms /    42 tokens (   71.57 ms per token,    13.97 tokens per second)\n",
      "llama_print_timings:        eval time =   32535.89 ms /    38 runs   (  856.21 ms per token,     1.17 tokens per second)\n",
      "llama_print_timings:       total time =   35962.85 ms /    80 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-46410a55-f942-4e1a-a967-7d4c169a28ac', 'object': 'text_completion', 'created': 1720597322, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': 'Q: name 3 species of fish? A: 1)Sturegon, a freshwater fish. 2)Barracuda, a saltwater fish. 3)Salmon, both a freshwater and saltwater fish. Q: Name 5 species of fish? A: 1)Catfish, a freshwater fish. 2)Trout, a freshwater fish. 4)Tuna, a saltwater fish. 5)Shark, a saltwater fish. ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 56, 'completion_tokens': 39, 'total_tokens': 95}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: name 3 species of fish? A: 1)Sturegon, a freshwater fish. 2)Barracuda, a saltwater fish. 3)Salmon, both a freshwater and saltwater fish. Q: Name 5 species of fish? A:\", # Prompt\n",
    "      max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bb24ee-6666-400a-b36b-4e17f0daba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1598.10 ms\n",
      "llama_print_timings:      sample time =      73.92 ms /     8 runs   (    9.24 ms per token,   108.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4361.74 ms /    35 tokens (  124.62 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5551.30 ms /     7 runs   (  793.04 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time =   10013.69 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-71989035-6bd3-4e44-86cf-68ffffe6f602', 'object': 'text_completion', 'created': 1720604923, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': 'Q: what is 4.0 times 5.02? A: 20.08 Q: what is 5.0 times 8.2? A: 15.2', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 38, 'completion_tokens': 8, 'total_tokens': 46}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: what is 4.0 times 5.02? A: 20.08 Q: what is 5.0 times 8.2?\", # Prompt\n",
    "      max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1894bac5-94cf-4963-9e61-de3a1e1bff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1598.10 ms\n",
      "llama_print_timings:      sample time =      60.85 ms /     7 runs   (    8.69 ms per token,   115.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1778.23 ms /    11 tokens (  161.66 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4366.19 ms /     6 runs   (  727.70 ms per token,     1.37 tokens per second)\n",
      "llama_print_timings:       total time =    6224.38 ms /    17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-2643f2cc-6027-49a9-a467-95e29c7b8611', 'object': 'text_completion', 'created': 1720605347, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': 'Q: what is 2.05 x 10.20? Answer=20.91 Q: what is 3.2 x 6.2? Answer=19.84 Q: what is 6.7 x 8.4? Answer=56.28 ', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 62, 'completion_tokens': 7, 'total_tokens': 69}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: what is 2.05 x 10.20? Answer=20.91 Q: what is 3.2 x 6.2? Answer=19.84 Q: what is 6.7 x 8.4? Answer=\", # Prompt\n",
    "      max_tokens=None, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91406a3b-75f0-47f9-855f-d681286704f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '14', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' Photos'\n",
      "b'ynthesis'\n",
      "b' is'\n",
      "b' the'\n",
      "b' process'\n",
      "b' by'\n",
      "b' which'\n",
      "b' plants'\n",
      "b','\n",
      "b' algae'\n",
      "b','\n",
      "b' and'\n",
      "b' some'\n",
      "b' bacteria'\n",
      "b' convert'\n",
      "b' light'\n",
      "b' energy'\n",
      "b' from'\n",
      "b' the'\n",
      "b' sun'\n",
      "b' into'\n",
      "b' chemical'\n",
      "b' energy'\n",
      "b' in'\n",
      "b' the'\n",
      "b' form'\n",
      "b' of'\n",
      "b' glucose'\n",
      "b' ('\n",
      "b'a'\n",
      "b' type'\n",
      "b' of'\n",
      "b' sugar'\n",
      "b').'\n",
      "b' This'\n",
      "b' process'\n",
      "b' occurs'\n",
      "b' in'\n",
      "b' specialized'\n",
      "b' org'\n",
      "b'anel'\n",
      "b'les'\n",
      "b' called'\n",
      "b' chlor'\n",
      "b'oplast'\n",
      "b's'\n",
      "b','\n",
      "b' which'\n",
      "b' contain'\n",
      "b' the'\n",
      "b' pigment'\n",
      "b' chlor'\n",
      "b'oph'\n",
      "b'yll'\n",
      "b'.\\n'\n",
      "b'Photos'\n",
      "b'ynthesis'\n",
      "b' can'\n",
      "b' be'\n",
      "b' divided'\n",
      "b' into'\n",
      "b' two'\n",
      "b' stages'\n",
      "b':'\n",
      "b' the'\n",
      "b' light'\n",
      "b'-dependent'\n",
      "b' reactions'\n",
      "b' and'\n",
      "b' the'\n",
      "b' light'\n",
      "b'-independent'\n",
      "b' reactions'\n",
      "b'.\\n\\n'\n",
      "b'1'\n",
      "b'.'\n",
      "b' Light'\n",
      "b'-'\n",
      "b'Dep'\n",
      "b'endent'\n",
      "b' Re'\n",
      "b'actions'\n",
      "b':\\n'\n",
      "b'The'\n",
      "b' light'\n",
      "b'-dependent'\n",
      "b' reactions'\n",
      "b' occur'\n",
      "b' in'\n",
      "b' the'\n",
      "b' th'\n",
      "b'yl'\n",
      "b'ak'\n",
      "b'oid'\n",
      "b' membranes'\n",
      "b' of'\n",
      "b' the'\n",
      "b' chlor'\n",
      "b'oplast'\n",
      "b' and'\n",
      "b' involve'\n",
      "b' the'\n",
      "b' conversion'\n",
      "b' of'\n",
      "b' light'\n",
      "b' energy'\n",
      "b' into'\n",
      "b' ATP'\n",
      "b' and'\n",
      "b' N'\n",
      "b'AD'\n",
      "b'PH'\n",
      "b'.'\n",
      "b' These'\n",
      "b' reactions'\n",
      "b' are'\n",
      "b' also'\n",
      "b' known'\n",
      "b' as'\n",
      "b' the'\n",
      "b' Hill'\n",
      "b' reaction'\n",
      "b'.\\n'\n",
      "b'\\t'\n",
      "b'*'\n",
      "b' Light'\n",
      "b' absorption'\n",
      "b':'\n",
      "b' Chlor'\n",
      "b'oph'\n",
      "b'yll'\n",
      "b' and'\n",
      "b' other'\n",
      "b' pig'\n",
      "b'ments'\n",
      "b' absorb'\n",
      "b' light'\n",
      "b' energy'\n",
      "b' from'\n",
      "b' the'\n",
      "b' sun'\n",
      "b'.\\n'\n",
      "b'\\t'\n",
      "b'*'\n",
      "b' Exc'\n",
      "b'itation'\n",
      "b' of'\n",
      "b' electrons'\n",
      "b':'\n",
      "b' The'\n",
      "b' absorbed'\n",
      "b' light'\n",
      "b' energy'\n",
      "b' exc'\n",
      "b'ites'\n",
      "b' electrons'\n",
      "b' in'\n",
      "b' the'\n",
      "b' pigment'\n",
      "b' molecules'\n",
      "b','\n",
      "b' which'\n",
      "b' are'\n",
      "b' then'\n",
      "b' transferred'\n",
      "b' to'\n",
      "b' a'\n",
      "b' special'\n",
      "b' molecule'\n",
      "b' called'\n",
      "b' an'\n",
      "b' electron'\n",
      "b' accept'\n",
      "b'or'\n",
      "b'.\\n'\n",
      "b'\\t'\n",
      "b'*'\n",
      "b' Transfer'\n",
      "b' of'\n",
      "b' electrons'\n",
      "b':'\n",
      "b' The'\n",
      "b' electrons'\n",
      "b' are'\n",
      "b' passed'\n",
      "b' along'\n",
      "b' a'\n",
      "b' series'\n",
      "b' of'\n",
      "b' protein'\n",
      "b' complexes'\n",
      "b' called'\n",
      "b' the'\n",
      "b' photos'\n",
      "b'yn'\n",
      "b'thetic'\n",
      "b' reaction'\n",
      "b' center'\n",
      "b','\n",
      "b' ultimately'\n",
      "b' resulting'\n",
      "b' in'\n",
      "b' the'\n",
      "b' formation'\n",
      "b' of'\n",
      "b' ATP'\n",
      "b' and'\n",
      "b' N'\n",
      "b'AD'\n",
      "b'PH'\n",
      "b'.\\n\\n'\n",
      "b'2'\n",
      "b'.'\n",
      "b' Light'\n",
      "b'-'\n",
      "b'Independent'\n",
      "b' Re'\n",
      "b'actions'\n",
      "b' ('\n",
      "b'Cal'\n",
      "b'vin'\n",
      "b' Cycle'\n",
      "b'):\\n'\n",
      "b'The'\n",
      "b' light'\n",
      "b'-independent'\n",
      "b' reactions'\n",
      "b' occur'\n",
      "b' in'\n",
      "b' the'\n",
      "b' st'\n",
      "b'roma'\n",
      "b' of'\n",
      "b' the'\n",
      "b' chlor'\n",
      "b'oplast'\n",
      "b' and'\n",
      "b' involve'\n",
      "b' the'\n",
      "b' fixation'\n",
      "b' of'\n",
      "b' CO'\n",
      "b'2'\n",
      "b' into'\n",
      "b' glucose'\n",
      "b' using'\n",
      "b' the'\n",
      "b' energy'\n",
      "b' from'\n",
      "b' ATP'\n",
      "b' and'\n",
      "b' N'\n",
      "b'AD'\n",
      "b'PH'\n",
      "b' produced'\n",
      "b' in'\n",
      "b' the'\n",
      "b' light'\n",
      "b'-dependent'\n",
      "b' reactions'\n",
      "b'.\\n'\n",
      "b'\\t'\n",
      "b'*'\n",
      "b' Carbon'\n",
      "b' fixation'\n",
      "b':'\n",
      "b' CO'\n",
      "b'2'\n",
      "b' is'\n",
      "b' fixed'\n",
      "b' into'\n",
      "b' a'\n",
      "b' '\n",
      "b'3'\n",
      "b'-car'\n",
      "b'bon'\n",
      "b' molecule'\n",
      "b' called'\n",
      "b' '\n",
      "b'3'\n",
      "b'-ph'\n",
      "b'osph'\n",
      "b'og'\n",
      "b'lycer'\n",
      "b'ate'\n",
      "b' ('\n",
      "b'3'\n",
      "b'-P'\n",
      "b'GA'\n",
      "b')'\n",
      "b' via'\n",
      "b' the'\n",
      "b' enzyme'\n",
      "b' Ru'\n",
      "b'B'\n",
      "b'is'\n",
      "b'CO'\n",
      "b'.\\n'\n",
      "b'\\t'\n",
      "b'*'\n",
      "b' Reduction'\n",
      "b' reactions'\n",
      "b':'\n",
      "b' The'\n",
      "b' '\n",
      "b'3'\n",
      "b'-P'\n",
      "b'GA'\n",
      "b' molecules'\n",
      "b' are'\n",
      "b' reduced'\n",
      "b' to'\n",
      "b' form'\n",
      "b' glyc'\n",
      "b'eral'\n",
      "b'dehyde'\n",
      "b'-'\n",
      "b'3'\n",
      "b'-ph'\n",
      "b'osphate'\n",
      "b' ('\n",
      "b'G'\n",
      "b'3'\n",
      "b'P'\n",
      "b')'\n",
      "b' using'\n",
      "b' the'\n",
      "b' energy'\n",
      "b' from'\n",
      "b' ATP'\n",
      "b' and'\n",
      "b' N'\n",
      "b'AD'\n",
      "b'PH'\n",
      "b' produced'\n",
      "b' in'\n",
      "b' the'\n",
      "b' light'\n",
      "b'-dependent'\n",
      "b' reactions'\n",
      "b'.\\n'\n",
      "b'\\t'\n",
      "b'*'\n",
      "b' Reg'\n",
      "b'eneration'\n",
      "b' of'\n",
      "b' Ru'\n",
      "b'BP'\n",
      "b':'\n",
      "b' The'\n",
      "b' G'\n",
      "b'3'\n",
      "b'P'\n",
      "b' molecules'\n",
      "b' are'\n",
      "b' used'\n",
      "b' to'\n",
      "b' regenerate'\n",
      "b' the'\n",
      "b' Ru'\n",
      "b'BP'\n",
      "b' molecule'\n",
      "b','\n",
      "b' which'\n",
      "b' is'\n",
      "b' necessary'\n",
      "b' for'\n",
      "b' carbon'\n",
      "b' fixation'\n",
      "b'.\\n\\n'\n",
      "b'Overall'\n",
      "b','\n",
      "b' photos'\n",
      "b'ynthesis'\n",
      "b' is'\n",
      "b' a'\n",
      "b' complex'\n",
      "b' process'\n",
      "b' that'\n",
      "b' involves'\n",
      "b' the'\n",
      "b' conversion'\n",
      "b' of'\n",
      "b' light'\n",
      "b' energy'\n",
      "b' into'\n",
      "b' chemical'\n",
      "b' energy'\n",
      "b'.'\n",
      "b' It'\n",
      "b' is'\n",
      "b' essential'\n",
      "b' for'\n",
      "b' life'\n",
      "b' on'\n",
      "b' Earth'\n",
      "b' as'\n",
      "b' it'\n",
      "b' provides'\n",
      "b' the'\n",
      "b' energy'\n",
      "b' and'\n",
      "b' organic'\n",
      "b' compounds'\n",
      "b' needed'\n",
      "b' to'\n",
      "b' support'\n",
      "b' the'\n",
      "b' food'\n",
      "b' chain'\n",
      "b'.\\n\\n'\n",
      "b'Here'\n",
      "b\"'s\"\n",
      "b' a'\n",
      "b' simple'\n",
      "b' equation'\n",
      "b' to'\n",
      "b' summarize'\n",
      "b' the'\n",
      "b' overall'\n",
      "b' process'\n",
      "b':\\n\\n'\n",
      "b'6'\n",
      "b' CO'\n",
      "b'2'\n",
      "b' +'\n",
      "b' '\n",
      "b'6'\n",
      "b' H'\n",
      "b'2'\n",
      "b'O'\n",
      "b' +'\n",
      "b' light'\n",
      "b' energy'\n",
      "b' \\xe2\\x86\\x92'\n",
      "b' C'\n",
      "b'6'\n",
      "b'H'\n",
      "b'12'\n",
      "b'O'\n",
      "b'6'\n",
      "b' ('\n",
      "b'gl'\n",
      "b'ucose'\n",
      "b')'\n",
      "b' +'\n",
      "b' '\n",
      "b'6'\n",
      "b' O'\n",
      "b'2'\n",
      "b'\\n\\n'\n",
      "b'In'\n",
      "b' this'\n",
      "b' equation'\n",
      "b','\n",
      "b' carbon'\n",
      "b' dioxide'\n",
      "b' ('\n",
      "b'CO'\n",
      "b'2'\n",
      "b')'\n",
      "b' and'\n",
      "b' water'\n",
      "b' ('\n",
      "b'H'\n",
      "b'2'\n",
      "b'O'\n",
      "b')'\n",
      "b' are'\n",
      "b' converted'\n",
      "b' into'\n",
      "b' glucose'\n",
      "b' ('\n",
      "b'C'\n",
      "b'6'\n",
      "b'H'\n",
      "b'12'\n",
      "b'O'\n",
      "b'6'\n",
      "b'),'\n",
      "b' a'\n",
      "b' type'\n",
      "b' of'\n",
      "b' sugar'\n",
      "b','\n",
      "b' using'\n",
      "b' the'\n",
      "b' energy'\n",
      "b' from'\n",
      "b' light'\n",
      "b'.'\n",
      "b' Oxygen'\n",
      "b' ('\n",
      "b'O'\n",
      "b'2'\n",
      "b')'\n",
      "b' is'\n",
      "b' released'\n",
      "b' as'\n",
      "b' a'\n",
      "b' by'\n",
      "b'product'\n",
      "b' of'\n",
      "b' photos'\n",
      "b'ynthesis'\n",
      "b'.'\n",
      "b' This'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "llama_decode returned 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Llama(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplain about photosynthesis.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonPrac/env/lib/python3.12/site-packages/llama_cpp/llama.py:740\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    742\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    743\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    744\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    759\u001b[0m         )\n",
      "File \u001b[0;32m~/pythonPrac/env/lib/python3.12/site-packages/llama_cpp/llama.py:578\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    574\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    576\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    577\u001b[0m )\n\u001b[0;32m--> 578\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/pythonPrac/env/lib/python3.12/site-packages/llama_cpp/_internals.py:349\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    344\u001b[0m return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[1;32m    346\u001b[0m     batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: llama_decode returned 1"
     ]
    }
   ],
   "source": [
    "model = Llama(\"/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf\")\n",
    "tokens = model.tokenize(b\"explain about photosynthesis.\")\n",
    "for token in model.generate(tokens, top_k=40, top_p=0.95, temp=0.5, repeat_penalty=1.1):\n",
    "    print(model.detokenize([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a654e2c-ed33-4357-8b01-be4b0bca8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     335.36 ms /   100 runs   (    3.35 ms per token,   298.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   40784.85 ms /   100 runs   (  407.85 ms per token,     2.45 tokens per second)\n",
      "llama_print_timings:       total time =   41263.68 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-9c889a18-9268-46a7-87c0-54596b0260f2', 'object': 'text_completion', 'created': 1720607223, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': 'what is photosynthesis? Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose (a type of sugar). This process occurs in specialized organelles called chloroplasts, which are found in plant cells.\\nPhotosynthesis can be divided into two stages:\\nLight-dependent reactions: These reactions occur in the thylakoid membranes of the chloroplast and involve the conversion of light energy into ATP and NADPH. The light-dependent reactions produce', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 6, 'completion_tokens': 100, 'total_tokens': 106}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"what is photosynthesis?\",max_tokens=100,echo=True,temperature=0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aaf8f27-47de-41ce-9a55-c0313cf78cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     346.58 ms /   100 runs   (    3.47 ms per token,   288.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   41838.02 ms /   100 runs   (  418.38 ms per token,     2.39 tokens per second)\n",
      "llama_print_timings:       total time =   42333.81 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-4c383cae-5e92-4ae9-9b56-8d7ca6360e3d', 'object': 'text_completion', 'created': 1720607323, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': 'what is photosynthesis? Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose (a type of sugar). This process occurs in specialized organelles called chloroplasts, which contain pigments such as chlorophyll that absorb light energy.\\nThe overall equation for photosynthesis is:\\n6 CO2 + 6 H2O + light energy → C6H12O6 (glucose) + 6 O2\\n\\nPhotosynthesis', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 6, 'completion_tokens': 100, 'total_tokens': 106}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"what is photosynthesis?\",max_tokens=100,echo=True,temperature=1.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd48fdd-b602-475f-a15f-d98733b6a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     350.32 ms /   100 runs   (    3.50 ms per token,   285.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3498.91 ms /    16 tokens (  218.68 ms per token,     4.57 tokens per second)\n",
      "llama_print_timings:        eval time =   40942.74 ms /    99 runs   (  413.56 ms per token,     2.42 tokens per second)\n",
      "llama_print_timings:       total time =   44939.18 ms /   115 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-0a5a135f-faef-48a8-b3de-ad813ee6abfb', 'object': 'text_completion', 'created': 1720607491, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': 'write a short story on missing a friend\\'s birthday party in 80 words. \\nAs I sat at home, scrolling through social media, I couldn\\'t help but feel guilty for missing my best friend\\'s birthday party. We had been inseparable since childhood, and I had let him down by not attending his special day. The memories of our laughter and adventures flashed before my eyes, and I wished I could turn back time to make it right. A simple \"Happy Birthday\" message wouldn\\'t be enough; I needed to make it up to him. I vowed to plan a', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 100, 'total_tokens': 117}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story on missing a friend's birthday party in 80 words.\",max_tokens=100,echo=True,temperature=0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55db97b7-bbc0-4540-9847-40413080a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     336.17 ms /   100 runs   (    3.36 ms per token,   297.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3183.72 ms /    16 tokens (  198.98 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:        eval time =   39764.11 ms /    99 runs   (  401.66 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:       total time =   43424.10 ms /   115 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-d4311c11-6a53-4f5c-9e4f-fcfb9bc8fd9f', 'object': 'text_completion', 'created': 1720607620, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': \"write a short story on missing a friend's birthday party in 80 words. \\nAs I walked home from work, my mind was preoccupied with the task ahead of me. It wasn't until I received a call from Emma that I realized today was her birthday. My heart sank as I thought about all the fun we could have been having right now if only I had remembered. The sound of her laughter and happy chatter echoed in my ears, a constant reminder of what I missed. I promised myself to make it up to her soon, but for now, all I could\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 100, 'total_tokens': 117}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story on missing a friend's birthday party in 80 words.\",max_tokens=100,echo=True,temperature=1.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a0bb922-107a-4f21-8627-1941da9fdf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     103.42 ms /    30 runs   (    3.45 ms per token,   290.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   12749.47 ms /    30 runs   (  424.98 ms per token,     2.35 tokens per second)\n",
      "llama_print_timings:       total time =   12893.79 ms /    30 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-0d3b4bdf-382e-42b6-96be-d833bc57b361', 'object': 'text_completion', 'created': 1720607949, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': \"write a short story on missing a friend's birthday party in 80 words. \\nIt was supposed to be a night to remember, my best friend's birthday party. But I woke up late, overslept and missed the\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 30, 'total_tokens': 47}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story on missing a friend's birthday party in 80 words.\",max_tokens=30,echo=True,temperature=1.0,top_k=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd7b074b-02ef-4fc2-b4c6-c9822c62019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     355.44 ms /   100 runs   (    3.55 ms per token,   281.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   41865.58 ms /   100 runs   (  418.66 ms per token,     2.39 tokens per second)\n",
      "llama_print_timings:       total time =   42365.89 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-d63048fa-d78b-4c37-9f56-f56522db6c9d', 'object': 'text_completion', 'created': 1720607997, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': \"write a short story on missing a friend's birthday party in 80 words. \\nAs I stood at the entrance of the party, I could hear the laughter and music drifting from inside. It was too late, I had already missed my best friend's birthday celebration. The thought of his disappointment weighed heavily on me. I felt guilty for not making it earlier, for being so caught up in my own life. I took a deep breath and decided to make amends by sending him a heartfelt apology message instead. (80 words)  Read More\\nwrite a short story on\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 100, 'total_tokens': 117}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story on missing a friend's birthday party in 80 words.\",max_tokens=100,echo=True,temperature=1.0,top_k=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f6a31bc-a6a0-4810-aabd-4298fcc79645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3081.41 ms\n",
      "llama_print_timings:      sample time =     399.10 ms /   100 runs   (    3.99 ms per token,   250.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   41738.80 ms /   100 runs   (  417.39 ms per token,     2.40 tokens per second)\n",
      "llama_print_timings:       total time =   42752.63 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-ec0d3496-6720-41a9-b591-7de3897fe98a', 'object': 'text_completion', 'created': 1720608240, 'model': '/home/technoidentity/Downloads/Meta-Llama-3-8B-Instruct.Q4_K_S.gguf', 'choices': [{'text': \"write a short story on missing a friend's birthday party in 80 words. \\nAs I sat at my desk, staring blankly at the clock, I couldn't help but think about the party I had just missed. My best friend's birthday, and I wasn't there to celebrate with her. A sudden surge of regret washed over me as I thought about all the memories we could have made that day. I wished I could turn back time and be there for her special day. But alas, it was too late now. All I could do was send a belated\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 100, 'total_tokens': 117}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story on missing a friend's birthday party in 80 words.\",max_tokens=100,echo=True,temperature=0.5,top_k=1000)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6980f078-8655-44f2-b8bf-48a6525913f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /home/technoidentity/Downloads/gemma-2b.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.name str              = gemma-2b\n",
      "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
      "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - type  f32:  164 tensors\n",
      "llm_load_vocab: special tokens cache size = 388\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256128\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_layer          = 18\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 256\n",
      "llm_load_print_meta: n_embd_v_gqa     = 256\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
      "llm_load_print_meta: model params     = 2.51 B\n",
      "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = gemma-2b\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 93\n",
      "llm_load_tensors: ggml ctx size =    0.08 MiB\n",
      "llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",
      ".............................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n",
      "llama_new_context_with_model: graph nodes  = 601\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\"/home/technoidentity/Downloads/gemma-2b.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de8d6957-dfc9-4df8-94a6-8ad98b2e5c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3075.58 ms\n",
      "llama_print_timings:      sample time =     916.50 ms /   100 runs   (    9.17 ms per token,   109.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3075.32 ms /    11 tokens (  279.57 ms per token,     3.58 tokens per second)\n",
      "llama_print_timings:        eval time =   73799.90 ms /    99 runs   (  745.45 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:       total time =   78162.52 ms /   110 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-03578103-cf47-457e-8365-bc5a0ebf8bfb', 'object': 'text_completion', 'created': 1720608576, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': \", and give your name to the editor.\\n\\nThe following is an example of a short story that was published in a magazine:\\n\\n<h1>The Gardener</h1>\\nOne morning, I got up early and walked through the garden with my wife. The grass was still wet from the night's rain. We had just planted some new flowers and plants, and we were waiting for them to grow. As we walked around the garden, we saw a bird sitting on a branch of a tree. It was\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 11, 'completion_tokens': 100, 'total_tokens': 111}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story of a day spent on gardening\",max_tokens=100,temperature=0.5,top_k=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3640170-a2f4-4c95-b34b-c0ed8cf3d21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3075.58 ms\n",
      "llama_print_timings:      sample time =     140.68 ms /    16 runs   (    8.79 ms per token,   113.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   12171.44 ms /    16 runs   (  760.72 ms per token,     1.31 tokens per second)\n",
      "llama_print_timings:       total time =   12362.40 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-b6090465-97eb-41e6-a1f8-76dcd634e045', 'object': 'text_completion', 'created': 1720608711, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': ', where you describe how the garden grows in spring or summer and then die during', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 11, 'completion_tokens': 16, 'total_tokens': 27}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story of a day spent on gardening\",temperature=1.0,top_k=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e329d6c1-80a7-4a0e-bd88-5d584f28716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3075.58 ms\n",
      "llama_print_timings:      sample time =     894.84 ms /   100 runs   (    8.95 ms per token,   111.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =   74293.65 ms /   100 runs   (  742.94 ms per token,     1.35 tokens per second)\n",
      "llama_print_timings:       total time =   75486.70 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-81382920-92c4-4379-957c-493f7b2eb1da', 'object': 'text_completion', 'created': 1720608744, 'model': '/home/technoidentity/Downloads/gemma-2b.gguf', 'choices': [{'text': \". The following is an example:\\n\\n<h1>Example 1</h1>\\nOn the third day, I went to my grandmother's house for lunch. After lunch, I helped my grandmother in her garden. She showed me how to plant some vegetables. It was very interesting. I liked it very much.\\n\\n<h1>Example 2</h1>\\nOn the fourth day, I visited my sister. She lives in a village. I had not seen her for a long time. We went to our grandmother'\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 11, 'completion_tokens': 100, 'total_tokens': 111}}\n"
     ]
    }
   ],
   "source": [
    "output = model(\"write a short story of a day spent on gardening\",max_tokens=100,temperature=0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9802c7-2618-43f8-aa4b-44aa6f5f6bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
